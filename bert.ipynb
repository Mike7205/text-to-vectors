{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Text classification of clickbait headlines\n",
    "## Transformer models: DistilBERT\n",
    "\n",
    "Transformer models represent words through taking into account their meaning (through word embeddings), their position in a sequence, and the amount of attention the model needs to pay to other words in the sequence in order to represent its contextual meaning. BERT and its offshoots are general purpose language models which can be fine tuned to do a number of natural language tasks such as text summarisation, question answering, grammar correction and text classification.\n",
    "\n",
    "## Load in dependencies and data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# https://medium.com/geekculture/hugging-face-distilbert-tensorflow-for-custom-text-classification-1ad4a49e26a7\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from transformers import (\n",
    "    DataCollatorWithPadding,\n",
    "    create_optimizer,\n",
    "    TFAutoModelForSequenceClassification\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Read in train and validation sets\n",
    "clickbait_train = pd.read_csv(\"data/clickbait_train.csv\", sep=\"\\t\", header=0)\n",
    "clickbait_val = pd.read_csv(\"data/clickbait_val.csv\", sep=\"\\t\", header=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Convert Pandas DataFrame into Dataset format"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "clickbait_train_ds = Dataset.from_pandas(clickbait_train)\n",
    "clickbait_val_ds = Dataset.from_pandas(clickbait_val)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tokenise data\n",
    "\n",
    "In Transformer models, raw text is taken in, tokenised, and converted to an ID which matches a vocabulary value in the pretrained model. This is done by calling the `Autotokenizer` method with the corresponding model you want to fine tune. We will be using [DistilBERT](https://huggingface.co/docs/transformers/model_doc/distilbert), a smaller, lighter version of BERT that preserves 95% of BERT's performance on many NLP tasks."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Read in the AutoTokeniser associated with our model\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def preprocess_function(rows):\n",
    "    return tokenizer(rows[\"text\"], padding=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/20 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "42e89b98fdba4e328c88fb65caa6c3c1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/7 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d729908188594f6f88743ce34443e560"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenise the train and validation data using the AutoTokeniser\n",
    "tokenized_train = clickbait_train_ds.map(preprocess_function, batched=True)\n",
    "tokenized_val = clickbait_val_ds.map(preprocess_function, batched=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "{'text': 'New insulin-resistance discovery may help diabetes sufferers',\n 'label': 0,\n 'input_ids': [101,\n  2047,\n  22597,\n  1011,\n  5012,\n  5456,\n  2089,\n  2393,\n  14671,\n  9015,\n  2545,\n  102,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0],\n 'attention_mask': [1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0]}"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print out an example of the tokenised text.\n",
    "tokenized_train[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The two values output during tokenisation that we need are the `input_ids` and the `attention_mask`. These are explained in more detail in [this](https://www.youtube.com/watch?v=Yffk5aydLzg&t=16s) and [this](https://www.youtube.com/watch?v=M6adb1j2jPI&t=166s) video. Let's start by examining the `input_ids`.\n",
    "\n",
    "Our DistilBERT model accepts raw text as an input, and retains punctuation as tokens. In addition, it splits some words into stems and prefixes, a little like what we did with lemmatisation."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['new', 'insulin', '-', 'resistance', 'discovery', 'may', 'help', 'diabetes', 'suffer', '##ers']\n"
     ]
    }
   ],
   "source": [
    "# Tokenising a headline into words\n",
    "tokens = tokenizer.tokenize(clickbait_train[\"text\"][0])\n",
    "print(tokens)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "These tokens are then mapped to an ID, based on a dictionary that was created during DistilBERT's training."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2047, 22597, 1011, 5012, 5456, 2089, 2393, 14671, 9015, 2545]\n"
     ]
    }
   ],
   "source": [
    "# Mapping the tokens to their IDs in the model\n",
    "input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(input_ids)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "However, as we know from previous methods we've talked about with text processing, all inputs must be the same length. It doesn't take long for us to find an example of two sentences which are very different lengths."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw sentences\n",
      "['irish', 'developer', 'found', 'dead', 'in', 'his', 'home']\n",
      "['boat', 'accident', 'in', 'democratic', 'republic', 'of', 'the', 'congo', 'kills', 'at', 'least', '73']\n",
      "\n",
      "Converted to IDs\n",
      "[3493, 9722, 2179, 2757, 1999, 2010, 2188]\n",
      "[4049, 4926, 1999, 3537, 3072, 1997, 1996, 9030, 8563, 2012, 2560, 6421]\n"
     ]
    }
   ],
   "source": [
    "# Extract two sentences with very different lengths\n",
    "different_length_sentences = clickbait_train[\"text\"][3:5]\n",
    "\n",
    "# Print out their tokens\n",
    "print(\"Raw sentences\")\n",
    "tokens = [tokenizer.tokenize(sentence) for sentence in different_length_sentences.tolist()]\n",
    "print(tokens[0])\n",
    "print(tokens[1])\n",
    "\n",
    "# Print out the input ID mappings\n",
    "print(\"\\nConverted to IDs\")\n",
    "ids = [tokenizer.convert_tokens_to_ids(token) for token in tokens]\n",
    "print(ids[0])\n",
    "print(ids[1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "What the tokeniser allows you to do is apply padding so that shorter sequences have the same length as longer ones. As you can see here, the shorter sentences has been padded out with zeros to make it the same length as the longer one."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 3493, 9722, 2179, 2757, 1999, 2010, 2188, 102, 0, 0, 0, 0, 0]\n",
      "[101, 4049, 4926, 1999, 3537, 3072, 1997, 1996, 9030, 8563, 2012, 2560, 6421, 102]\n"
     ]
    }
   ],
   "source": [
    "# Show the two sentences above with padding added - they are now the same length!\n",
    "padded_tokenizer = tokenizer(different_length_sentences.tolist(), padding = True)\n",
    "print(padded_tokenizer[\"input_ids\"][0])\n",
    "print(padded_tokenizer[\"input_ids\"][1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "However, there is one remaining issue. The attention mechanism within the model doesn't understand that these padded IDs don't mean anything, and if we don't instruct the model to ignore them, it will distort the model predictions. As such, when padding is applied, attention masks are generated for each sentence. These are vectors of the same length as the input vector, with 1's to tell the model to use this token, and 0's to tell it to ignore it. We can see that the attention mask for sentence 1 is instructing the model to ignore all the padded tokens."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1\n",
      "[101, 3493, 9722, 2179, 2757, 1999, 2010, 2188, 102, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n",
      "\n",
      "Sentence 2\n",
      "[101, 4049, 4926, 1999, 3537, 3072, 1997, 1996, 9030, 8563, 2012, 2560, 6421, 102]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# Show how the attention mask works\n",
    "print(\"Sentence 1\")\n",
    "print(padded_tokenizer[\"input_ids\"][0])\n",
    "print(padded_tokenizer[\"attention_mask\"][0])\n",
    "\n",
    "print(\"\\nSentence 2\")\n",
    "print(padded_tokenizer[\"input_ids\"][1])\n",
    "print(padded_tokenizer[\"attention_mask\"][1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Convert Dataset into Tensors\n",
    "\n",
    "As we're using a Tensorflow model, we need to convert the Hugging Face Dataset into something that tensorflow can understand. That means we need to convert each sentence, with the input IDs, attention masks and labels into Tensorflow tensors. In order to make sure that the padding length is consistent across all three datasets, we can use a `DataCollatorWithPadding` to even this out before we get to model training."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# Create data collator, which will standardise the padding across all datasets to make\n",
    "# sure all inputs are the same length\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-05 09:35:03.941220: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Convert the train and validation sets into tensors\n",
    "tf_train_set = tokenized_train.to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"label\"],\n",
    "    shuffle=False,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "\n",
    "tf_val_set = tokenized_val.to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"label\"],\n",
    "    shuffle=False,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "When comparing this to the information contained in the Dataset, we have all the same fields: `input_ids`, `attention_mask` and `labels`. When we print out the first example, you can see that the information is also the same."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "<PrefetchDataset element_spec={'input_ids': TensorSpec(shape=(None, None), dtype=tf.int64, name=None), 'attention_mask': TensorSpec(shape=(None, None), dtype=tf.int64, name=None), 'labels': TensorSpec(shape=(None,), dtype=tf.int64, name=None)}>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_train_set"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[  101  2047 22597  1011  5012  5456  2089  2393 14671  9015  2545   102\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0], shape=(28,), dtype=int64)\n",
      "tf.Tensor([1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0], shape=(28,), dtype=int64)\n",
      "tf.Tensor(0, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for sentence in tf_train_set.take(1):\n",
    "    print(sentence[\"input_ids\"][0])\n",
    "    print(sentence[\"attention_mask\"][0])\n",
    "    print(sentence[\"labels\"][0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fine tuning the DistilBERT model\n",
    "\n",
    "We can now get to fine-tuning our DistilBERT model. We first read in the model using the `TFAutoModelForSequenceClassification` for sequence classification. What this tells the trainer to do is drop the final layer of the original DistilBERT model and add a layer with two outcomes. We'll train this layer in order to create our BERT-based clickbait classifier."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_projector', 'activation_13', 'vocab_layer_norm', 'vocab_transform']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['dropout_19', 'classifier', 'pre_classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Import the pretrained distilBERT model with a new final layer which we'll use for classifying clickbait titles\n",
    "bert_model = TFAutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In order to train the model, we need an optimiser. The below code allows us to create an optimiser that will decay the learning rate in line with the number of planned epochs."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "num_epochs = 3\n",
    "batches_per_epoch = len(tokenized_train) // batch_size\n",
    "total_train_steps = int(batches_per_epoch * num_epochs)\n",
    "optimizer, schedule = create_optimizer(init_lr=2e-5, num_warmup_steps=0, num_train_steps=total_train_steps)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
     ]
    }
   ],
   "source": [
    "bert_model.compile(optimizer=optimizer,\n",
    "                   metrics=[\"accuracy\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, we can fine-tune our model for clickbait classification!"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1200/1200 [==============================] - 885s 732ms/step - loss: 0.0716 - accuracy: 0.9777 - val_loss: 0.0318 - val_accuracy: 0.9892\n",
      "Epoch 2/3\n",
      "1200/1200 [==============================] - 951s 792ms/step - loss: 0.0128 - accuracy: 0.9960 - val_loss: 0.0326 - val_accuracy: 0.9900\n",
      "Epoch 3/3\n",
      "1200/1200 [==============================] - 934s 779ms/step - loss: 0.0037 - accuracy: 0.9991 - val_loss: 0.0383 - val_accuracy: 0.9895\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x14d68fe20>"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fine tune our model using the training data\n",
    "bert_model.fit(x=tf_train_set,\n",
    "               validation_data=tf_val_set,\n",
    "               epochs=3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400/400 [==============================] - 83s 204ms/step\n"
     ]
    }
   ],
   "source": [
    "# Generate model predictions\n",
    "preds = bert_model.predict(tf_val_set).logits\n",
    "pred_val_labels = np.argmax(preds, axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "# Add a column with the predictions to the validation data\n",
    "clickbait_val[\"bert_pred\"] = pred_val_labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clickbait_val' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[0;32mIn [1]\u001B[0m, in \u001B[0;36m<cell line: 2>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Headlines the model thought were not clickbait, but which are\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[43mclickbait_val\u001B[49m\u001B[38;5;241m.\u001B[39mloc[(clickbait_val[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m) \u001B[38;5;241m&\u001B[39m (clickbait_val[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbert_pred\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtext\u001B[39m\u001B[38;5;124m\"\u001B[39m][:\u001B[38;5;241m5\u001B[39m]\n",
      "\u001B[0;31mNameError\u001B[0m: name 'clickbait_val' is not defined"
     ]
    }
   ],
   "source": [
    "# Headlines the model thought were not clickbait, but which are\n",
    "clickbait_val.loc[(clickbait_val[\"label\"] == 1) & (clickbait_val[\"bert_pred\"] == 0), \"text\"][:5]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "123     Avenged Sevenfold drummer James \"The Rev\" Sull...\n422       Dolls Resembling Daughters Displease First Lady\n612                                     A Note to Readers\n1228    How Bethpage Black Was Mastered (For a Day) By...\n1237                 24 error messages sent by Flight 447\nName: text, dtype: object"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Headlines the model thought were clickbait, but which are not\n",
    "clickbait_val.loc[(clickbait_val[\"label\"] == 0) & (clickbait_val[\"bert_pred\"] == 1), \"text\"][:5]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}