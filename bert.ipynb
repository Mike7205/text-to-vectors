{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Text classification of clickbait headlines\n",
    "## Transformer models: DistilBERT\n",
    "\n",
    "Transformer models represent words through taking into account their meaning (through word embeddings), their position in a sequence, and the amount of attention the model needs to pay to other words in the sequence in order to represent its contextual meaning. BERT and its offshoots are general purpose language models which can be fine tuned to do a number of natural language tasks such as text summarisation, question answering, grammar correction and text classification.\n",
    "\n",
    "## Load in dependencies and data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "# https://medium.com/geekculture/hugging-face-distilbert-tensorflow-for-custom-text-classification-1ad4a49e26a7\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from transformers import (\n",
    "    DataCollatorWithPadding,\n",
    "    create_optimizer,\n",
    "    TFAutoModelForSequenceClassification\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "clickbait_train = pd.read_csv(\"data/clickbait_train.csv\", sep=\"\\t\", header=0)\n",
    "clickbait_val = pd.read_csv(\"data/clickbait_val.csv\", sep=\"\\t\", header=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Convert Pandas DataFrame into Dataset format"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "clickbait_train_ds = Dataset.from_pandas(clickbait_train)\n",
    "clickbait_val_ds = Dataset.from_pandas(clickbait_val)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tokenise data\n",
    "\n",
    "In Transformer models, raw text is taken in, tokenised, and converted to an ID which matches a vocabulary value in the pretrained model. This is done by calling the `Autotokenizer` method with the corresponding model you want to fine tune. We will be using [DistilBERT](https://huggingface.co/docs/transformers/model_doc/distilbert), a smaller, lighter version of BERT that preserves 95% of BERT's performance on many NLP tasks."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "def preprocess_function(rows):\n",
    "    return tokenizer(rows[\"text\"], padding=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/20 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "87628ffb31c248e08885d73cf5cd556f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/7 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "64602a1341374c12adf0b7a187bbd1da"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_train = clickbait_train_ds.map(preprocess_function, batched=True)\n",
    "tokenized_val = clickbait_val_ds.map(preprocess_function, batched=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "{'text': 'New insulin-resistance discovery may help diabetes sufferers',\n 'label': 0,\n 'input_ids': [101,\n  2047,\n  22597,\n  1011,\n  5012,\n  5456,\n  2089,\n  2393,\n  14671,\n  9015,\n  2545,\n  102,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0],\n 'attention_mask': [1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0]}"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The two values output during tokenisation that we need are the `input_ids` and the `attention_mask`. These are explained in more detail in [this](https://www.youtube.com/watch?v=Yffk5aydLzg&t=16s) and [this](https://www.youtube.com/watch?v=M6adb1j2jPI&t=166s) video. Let's start by examining the `input_ids`.\n",
    "\n",
    "Our DistilBERT model accepts raw text as an input, and retains punctuation as tokens. In addition, it splits some words into stems and prefixes, a little like what we did with lemmatisation."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['new', 'insulin', '-', 'resistance', 'discovery', 'may', 'help', 'diabetes', 'suffer', '##ers']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(clickbait_train[\"text\"][0])\n",
    "print(tokens)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "These tokens are then mapped to an ID, based on a dictionary that was created during DistilBERT's training."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2047, 22597, 1011, 5012, 5456, 2089, 2393, 14671, 9015, 2545]\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(input_ids)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, the special tokens 101 and 102 are added to the beginning and end of the sentence. These tell the model that this is the start and end of a sequence respectively."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 2047, 22597, 1011, 5012, 5456, 2089, 2393, 14671, 9015, 2545, 102]\n"
     ]
    }
   ],
   "source": [
    "final_inputs = tokenizer.prepare_for_model(input_ids)\n",
    "print(final_inputs[\"input_ids\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "However, as we know from previous methods we've talked about with text processing, all inputs must be the same length. It doesn't take long for us to find an example of two sentences which are very different lengths."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw sentences\n",
      "['irish', 'developer', 'found', 'dead', 'in', 'his', 'home']\n",
      "['boat', 'accident', 'in', 'democratic', 'republic', 'of', 'the', 'congo', 'kills', 'at', 'least', '73']\n",
      "\n",
      "Converted to IDs\n",
      "[3493, 9722, 2179, 2757, 1999, 2010, 2188]\n",
      "[4049, 4926, 1999, 3537, 3072, 1997, 1996, 9030, 8563, 2012, 2560, 6421]\n"
     ]
    }
   ],
   "source": [
    "different_length_sentences = clickbait_train[\"text\"][3:5]\n",
    "\n",
    "print(\"Raw sentences\")\n",
    "tokens = [tokenizer.tokenize(sentence) for sentence in different_length_sentences.tolist()]\n",
    "print(tokens[0])\n",
    "print(tokens[1])\n",
    "\n",
    "print(\"\\nConverted to IDs\")\n",
    "ids = [tokenizer.convert_tokens_to_ids(token) for token in tokens]\n",
    "print(ids[0])\n",
    "print(ids[1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "What the tokeniser allows you to do is apply padding so that shorter sequences have the same length as longer ones. As you can see here, the shorter sentences has been padded out with zeros to make it the same length as the longer one."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 3493, 9722, 2179, 2757, 1999, 2010, 2188, 102, 0, 0, 0, 0, 0]\n",
      "[101, 4049, 4926, 1999, 3537, 3072, 1997, 1996, 9030, 8563, 2012, 2560, 6421, 102]\n"
     ]
    }
   ],
   "source": [
    "padded_tokenizer = tokenizer(different_length_sentences.tolist(), padding = True)\n",
    "print(padded_tokenizer[\"input_ids\"][0])\n",
    "print(padded_tokenizer[\"input_ids\"][1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "However, there is one remaining issue. The attention mechanism within the model doesn't understand that these padded IDs don't mean anything, and if we don't instruct the model to ignore them, it will distort the model predictions. As such, when padding is applied, attention masks are generated for each sentence. These are vectors of the same length as the input vector, with 1's to tell the model to use this token, and 0's to tell it to ignore it. We can see that the attention mask for sentence 1 is instructing the model to ignore all the padded tokens."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1\n",
      "[101, 3493, 9722, 2179, 2757, 1999, 2010, 2188, 102, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n",
      "\n",
      "Sentence 2\n",
      "[101, 4049, 4926, 1999, 3537, 3072, 1997, 1996, 9030, 8563, 2012, 2560, 6421, 102]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(\"Sentence 1\")\n",
    "print(padded_tokenizer[\"input_ids\"][0])\n",
    "print(padded_tokenizer[\"attention_mask\"][0])\n",
    "\n",
    "print(\"\\nSentence 2\")\n",
    "print(padded_tokenizer[\"input_ids\"][1])\n",
    "print(padded_tokenizer[\"attention_mask\"][1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Convert Dataset into Tensors\n",
    "\n",
    "As we're using a Tensorflow model, we need to convert the Hugging Face Dataset into something that tensorflow can understand. That means we need to convert each sentence, with the input IDs, attention masks and labels into Tensorflow tensors. In order to make sure that the padding length is consistent across all three datasets, we can use a `DataCollatorWithPadding` to even this out before we get to model training."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-09 14:10:16.983624: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "tf_train_set = tokenized_train.to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"label\"],\n",
    "    shuffle=False,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "\n",
    "tf_val_set = tokenized_val.to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"label\"],\n",
    "    shuffle=False,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "When comparing this to the information contained in the Dataset, we have all the same fields: `input_ids`, `attention_mask` and `labels`. When we print out the first example, you can see that the information is also the same."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "<PrefetchDataset element_spec={'input_ids': TensorSpec(shape=(None, None), dtype=tf.int64, name=None), 'attention_mask': TensorSpec(shape=(None, None), dtype=tf.int64, name=None), 'labels': TensorSpec(shape=(None,), dtype=tf.int64, name=None)}>"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_train_set"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[  101  2047 22597  1011  5012  5456  2089  2393 14671  9015  2545   102\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0], shape=(28,), dtype=int64)\n",
      "tf.Tensor([1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0], shape=(28,), dtype=int64)\n",
      "tf.Tensor(0, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for sentence in tf_train_set.take(1):\n",
    "    print(sentence[\"input_ids\"][0])\n",
    "    print(sentence[\"attention_mask\"][0])\n",
    "    print(sentence[\"labels\"][0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fine tuning the DistilBERT model\n",
    "\n",
    "We can now get to fine-tuning our DistilBERT model. We first read in the model using the `TFAutoModelForSequenceClassification` for sequence classification. What this tells the trainer to do is drop the final layer of the original DistilBERT model and add a layer with two outcomes. We'll train this layer in order to create our BERT-based clickbait classifier."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_layer_norm', 'vocab_transform', 'vocab_projector', 'activation_13']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier', 'dropout_19', 'classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "bert_model = TFAutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In order to train the model, we need an optimiser. The below code allows us to create an optimiser that will decay the learning rate in line with the number of planned epochs."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "num_epochs = 3\n",
    "batches_per_epoch = len(tokenized_train) // batch_size\n",
    "total_train_steps = int(batches_per_epoch * num_epochs)\n",
    "optimizer, schedule = create_optimizer(init_lr=2e-5, num_warmup_steps=0, num_train_steps=total_train_steps)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
     ]
    }
   ],
   "source": [
    "bert_model.compile(optimizer=optimizer,\n",
    "                   metrics=[\"accuracy\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, we can fine-tune our model for clickbait classification!"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1200/1200 [==============================] - 1033s 855ms/step - loss: 0.0732 - accuracy: 0.9776 - val_loss: 0.0341 - val_accuracy: 0.9867\n",
      "Epoch 2/3\n",
      "1200/1200 [==============================] - 1241s 1s/step - loss: 0.0147 - accuracy: 0.9956 - val_loss: 0.0375 - val_accuracy: 0.9883\n",
      "Epoch 3/3\n",
      "1200/1200 [==============================] - 1191s 992ms/step - loss: 0.0038 - accuracy: 0.9992 - val_loss: 0.0392 - val_accuracy: 0.9894\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x151a21b50>"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_model.fit(x=tf_train_set,\n",
    "               validation_data=tf_val_set,\n",
    "               epochs=3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400/400 [==============================] - 106s 260ms/step\n"
     ]
    }
   ],
   "source": [
    "# Generate model predictions\n",
    "preds = bert_model.predict(tf_val_set).logits\n",
    "pred_val_labels = np.argmax(preds, axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3168   36]\n",
      " [  32 3164]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(clickbait_val[\"label\"], pred_val_labels))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "clickbait_val[\"bert_pred\"] = pred_val_labels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "83      Photographer Gregory Crewdson Releases Hauntin...\n222     Oscar-Nominated Movie Posters With White Actor...\n683          Richard Madden Looking Attractive On A Horse\n1397    Inside China's Memefacturing Factories, Where ...\n1644    A Dutch Organization Is Providing Free Abortio...\nName: text, dtype: object"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clickbait_val.loc[(clickbait_val[\"label\"] == 1) & (clickbait_val[\"bert_pred\"] == 0), \"text\"][:5]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "123     Avenged Sevenfold drummer James \"The Rev\" Sull...\n422       Dolls Resembling Daughters Displease First Lady\n612                                     A Note to Readers\n967         Add Nuts to Your Diet With Sauces, Not Snacks\n1228    How Bethpage Black Was Mastered (For a Day) By...\nName: text, dtype: object"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clickbait_val.loc[(clickbait_val[\"label\"] == 0) & (clickbait_val[\"bert_pred\"] == 1), \"text\"][:5]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}