{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Text classification of clickbait headlines\n",
    "## Word embeddings: word2vec\n",
    "\n",
    "Word embeddings are representations of each word's meaning, which are derived by examining the context that a word is used in across a large text corpus. The meanings are represented as n-dimensional vectors, which in this case will be derived from the hidden layer of a word2vec model. These embeddings can be compared to each other in an n-dimensional space, with words that have similar meaning in the training corpus ending up close together, while those with dissimilar meanings being far apart.\n",
    "\n",
    "## Load in dependencies and data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from support_functions import train_text_classification_model, generate_predictions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-07-26T11:30:13.083423Z",
     "start_time": "2023-07-26T11:30:13.076598Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "cwd = \"Users/jodie.burchell/Documents/git/text-to-vectors\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-26T11:37:02.075669Z",
     "start_time": "2023-07-26T11:37:02.068396Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Load in train and validation data\n",
    "clickbait_train = pd.read_csv(f\"{cwd}/data/clickbait_train.csv\", sep=\"\\t\", header=0)\n",
    "clickbait_val = pd.read_csv(f\"{cwd}/data/clickbait_val.csv\", sep=\"\\t\", header=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prepare data for word2vec training\n",
    "\n",
    "In order to get the data ready for word2vec training, we need to do a small amount of pre-preparation.\n",
    "\n",
    "Firstly, we do some light string cleaning, including converting all characters to lowercase, removing all numbers and punctuation, and removing additional whitespace. This is because word2vec models, like bag-of-words models, are based on word tokens, so we want to normalise the text as much as possible before creating the embeddings."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def apply_light_string_cleaning(dataset: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Cleans a string: converts all characters to lowercase, removes all non-alphanumeric characters and removes additional whitespace.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        dataset\n",
    "        .str.lower()\n",
    "        .str.replace(\"[\\W_]+\", \" \", regex=True)\n",
    "        .str.replace(\"\\s+\", \" \", regex=True)\n",
    "        .str.strip()\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Apply the string cleaning to the train and validation data\n",
    "clickbait_train[\"text_clean\"] = apply_light_string_cleaning(clickbait_train[\"text\"])\n",
    "clickbait_val[\"text_clean\"] = apply_light_string_cleaning(clickbait_val[\"text\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, we split each sentence into a list of words, the expected format for a word2vec model."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Convert sentences into list of lists for training\n",
    "clickbait_w2v_training = clickbait_train[\"text_clean\"].str.split(\"\\s\").to_list()\n",
    "\n",
    "# Remove nans\n",
    "clickbait_w2v_training = [s for s in clickbait_w2v_training if type(s) is list]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "['new',\n 'insulin',\n 'resistance',\n 'discovery',\n 'may',\n 'help',\n 'diabetes',\n 'sufferers']"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of the cleaned and converted clickbait headline\n",
    "clickbait_w2v_training[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train w2v model to get word embeddings"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Import gensim Word2Vec method\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Train word2vec model\n",
    "w2v_model = Word2Vec(sentences=clickbait_w2v_training,\n",
    "                     vector_size=100,\n",
    "                     window=5,\n",
    "                     min_count=2,\n",
    "                     workers=4,\n",
    "                     sg=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.88190663e-01  2.56053746e-01 -1.04162320e-01  8.60497132e-02\n",
      "  1.00232720e-01 -4.76431906e-01  4.36868221e-01  6.02400482e-01\n",
      " -5.12246609e-01 -5.40899277e-01  2.65833810e-02 -4.35974747e-01\n",
      "  1.53366581e-01  1.78240821e-01  5.79746664e-01 -5.21296030e-03\n",
      "  2.77763575e-01 -6.06821477e-03 -1.45945728e-01 -5.57567596e-01\n",
      "  7.00066388e-02  2.30736330e-01 -1.01522334e-01 -1.40830933e-04\n",
      "  1.05173676e-03 -8.62729475e-02 -1.89191237e-01 -2.32163504e-01\n",
      " -1.39115909e-02 -8.23591948e-02  6.46053910e-01 -1.72430322e-01\n",
      " -8.42885580e-03 -4.32763845e-01 -5.46497881e-01  4.41423714e-01\n",
      "  1.84663013e-01 -6.35258481e-02 -1.93889454e-01 -5.05963504e-01\n",
      " -1.45569175e-01  1.49315089e-01 -7.83952773e-02 -4.75108176e-02\n",
      "  2.12346032e-01 -6.14013262e-02 -1.68439135e-01 -4.50822860e-01\n",
      "  1.54646203e-01  1.44193724e-01  2.48018473e-01 -4.35005307e-01\n",
      "  6.89142719e-02 -6.29392207e-01 -2.33305573e-01 -2.27976829e-01\n",
      "  4.10295092e-02  7.56473839e-02  3.51655111e-02  3.00977468e-01\n",
      " -1.04890116e-01 -2.65275449e-01  5.46671569e-01 -1.50317714e-01\n",
      " -7.82205760e-02  5.99396110e-01 -1.35449871e-01  5.18702030e-01\n",
      " -3.81087303e-01  2.92898148e-01 -8.73931125e-02  9.82792079e-02\n",
      "  1.93186462e-01  2.44759247e-01  7.25090861e-01  2.08826974e-01\n",
      "  1.15793571e-01  3.65148991e-01 -1.01824045e-01 -2.57239670e-01\n",
      " -2.61087775e-01  3.19805816e-02 -2.50971198e-01  3.41278642e-01\n",
      "  5.00386581e-02 -3.30432028e-01  4.11779791e-01 -3.02209370e-02\n",
      "  3.37074816e-01 -3.32166404e-01  3.68099928e-01  9.52573940e-02\n",
      "  5.29782891e-01 -1.53782398e-01  5.21360815e-01  2.52033055e-01\n",
      "  4.34931666e-01 -3.34163666e-01  1.83557525e-01 -1.81832328e-01]\n"
     ]
    }
   ],
   "source": [
    "# Retrieve word embedding for \"best\"\n",
    "print(w2v_model.wv[\"best\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "[('worst', 0.9578117728233337),\n ('greatest', 0.950883150100708),\n ('funniest', 0.936430037021637),\n ('cutest', 0.9352415204048157),\n ('most', 0.9339564442634583),\n ('twitter', 0.9231436252593994),\n ('friend', 0.9160161018371582),\n ('cast', 0.9158626198768616),\n ('thing', 0.9145550727844238),\n ('absolute', 0.9111383557319641)]"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find words most similar to \"best\"\n",
    "w2v_model.wv.most_similar(\"best\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Extract vectors and average them across the documents"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def extract_document_vectors(model: Word2Vec, text: str, len_vectors: int):\n",
    "    \"\"\"\n",
    "    Takes in a clickbait headline, and iterates over every word in the sequence. For each word, it retrieves\n",
    "    its word embedding from the word2vec model, and then appends it to a NumPy array. Returns this array of\n",
    "    word embeddings.\n",
    "    \"\"\"\n",
    "    # Create empty NumPy array\n",
    "    vectors = np.empty((0, len_vectors), float)\n",
    "    # Loop over each word in clickbait headline\n",
    "    for word in text.split():\n",
    "        # Checks if word is in word2vec model\n",
    "        if word in model.wv.key_to_index:\n",
    "            # Retrieves embedding and appends it to the vectors array\n",
    "            v = model.wv[word]\n",
    "            vectors = np.append(vectors, np.array([v]), axis=0)\n",
    "    return vectors\n",
    "\n",
    "\n",
    "def calculate_w2v_dataset(model: Word2Vec, dataset: pd.DataFrame, len_vectors: int):\n",
    "    \"\"\"\n",
    "    Create a NumPy array which contains the average embedding for a headline, as well as the label\n",
    "    (whether it is clickbait or non-clickbait).\n",
    "    \"\"\"\n",
    "    # Create an empty NumPy array to contain the averaged headline vectors\n",
    "    document_vectors = np.empty((0, len_vectors), float)\n",
    "    # Create an empty NumPy array for the headline labels\n",
    "    matched_labels = []\n",
    "    # Iterate over the dataset containing the cleaned headline and the label\n",
    "    for index, row in dataset.iterrows():\n",
    "        # Extract the array of word embeddings for each headline\n",
    "        v = extract_document_vectors(model, row[\"text_clean\"], len_vectors)\n",
    "        # Check if the array is not empty\n",
    "        if v.shape[0] > 0:\n",
    "            # Average the array to yield one headline embedding\n",
    "            v_mean = v.mean(axis=0)\n",
    "            # Append the headline embedding and label\n",
    "            document_vectors = np.append(document_vectors, np.array([v_mean]), axis=0)\n",
    "            matched_labels.append(row[\"label\"])\n",
    "        else:\n",
    "            pass\n",
    "    return document_vectors, np.array(matched_labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# Extract the document embeddings for each dataset\n",
    "document_vectors_train, final_labels_train = calculate_w2v_dataset(w2v_model, clickbait_train, 100)\n",
    "document_vectors_val, final_labels_val = calculate_w2v_dataset(w2v_model, clickbait_val, 100)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document embedding for 'New insulin-resistance discovery may help diabetes sufferers'\n",
      "[-4.28797632e-02  1.90623574e-01  5.63060039e-02  1.70299198e-01\n",
      "  1.26286471e-02 -2.95280412e-01  7.38005054e-02  3.30918918e-01\n",
      " -9.69956213e-02 -1.26176431e-01 -9.31461652e-04 -2.19160682e-01\n",
      " -4.91200850e-02  1.05296968e-01  1.49029087e-02 -1.32236180e-01\n",
      "  1.81723403e-02 -1.46027402e-01  1.01369721e-01 -4.44048584e-01\n",
      "  1.26423729e-01  5.65344516e-02  9.69023287e-02 -1.00688658e-01\n",
      "  2.89612760e-03  9.48078775e-04 -3.84141317e-02 -9.09979551e-02\n",
      " -2.67467407e-01  1.18401208e-02  1.44312273e-01 -8.80012494e-02\n",
      "  1.29057348e-01 -1.83880170e-01 -2.65405855e-02  1.96613671e-01\n",
      "  9.31484954e-02 -2.29062152e-02 -7.69391339e-02 -2.46617089e-01\n",
      "  6.63064066e-02 -2.92988983e-01 -2.18627728e-01  1.14907216e-01\n",
      "  1.60316158e-01 -1.49795049e-01 -1.91479467e-01 -1.64924543e-02\n",
      "  1.70000556e-01  1.37826833e-01  8.17044700e-02 -2.00281414e-01\n",
      " -2.91220637e-02 -5.16959407e-02 -1.51485297e-01 -2.22565561e-02\n",
      "  8.91445776e-02 -9.56941663e-02 -1.33063765e-01  5.59008407e-02\n",
      "  4.24193331e-02  8.10703958e-02  5.07435985e-02 -8.21049170e-03\n",
      " -8.52020656e-02  1.25175467e-01 -9.23461641e-02  1.65537066e-01\n",
      " -1.00591240e-01  1.48127518e-01 -9.33838024e-02  2.03113571e-01\n",
      "  1.69996493e-01  6.94682634e-02  2.49825673e-01  1.26148855e-01\n",
      "  1.79648463e-02  3.81313109e-04  5.90136262e-02 -3.34589289e-02\n",
      " -1.35992035e-01 -8.42486514e-02 -1.30717768e-01  4.95796734e-02\n",
      " -2.97106470e-02 -6.89432225e-02  3.44641060e-02  5.41559230e-02\n",
      "  1.72007966e-01  1.21464744e-01  1.59433303e-01  1.78610846e-01\n",
      "  4.12502922e-02  4.10290554e-02  3.14687138e-01  1.52327285e-01\n",
      "  6.93286217e-02 -1.96150659e-01 -2.82223175e-02  7.12214603e-02]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Document embedding for '{clickbait_train['text'][0]}'\")\n",
    "print(document_vectors_train[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train clickbait classifier"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-12 13:25:38.947884: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "10/10 [==============================] - 1s 16ms/step - loss: 0.5904 - accuracy: 0.8086 - val_loss: 0.5084 - val_accuracy: 0.9216\n",
      "Epoch 2/20\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.4641 - accuracy: 0.9345 - val_loss: 0.4192 - val_accuracy: 0.9398\n",
      "Epoch 3/20\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.3886 - accuracy: 0.9453 - val_loss: 0.3556 - val_accuracy: 0.9480\n",
      "Epoch 4/20\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.3314 - accuracy: 0.9477 - val_loss: 0.3050 - val_accuracy: 0.9511\n",
      "Epoch 5/20\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.2857 - accuracy: 0.9492 - val_loss: 0.2645 - val_accuracy: 0.9505\n",
      "Epoch 6/20\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.2491 - accuracy: 0.9496 - val_loss: 0.2320 - val_accuracy: 0.9516\n",
      "Epoch 7/20\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.2201 - accuracy: 0.9501 - val_loss: 0.2061 - val_accuracy: 0.9506\n",
      "Epoch 8/20\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1969 - accuracy: 0.9507 - val_loss: 0.1854 - val_accuracy: 0.9514\n",
      "Epoch 9/20\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1787 - accuracy: 0.9510 - val_loss: 0.1694 - val_accuracy: 0.9513\n",
      "Epoch 10/20\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1643 - accuracy: 0.9507 - val_loss: 0.1566 - val_accuracy: 0.9520\n",
      "Epoch 11/20\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1532 - accuracy: 0.9506 - val_loss: 0.1467 - val_accuracy: 0.9517\n",
      "Epoch 12/20\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1446 - accuracy: 0.9509 - val_loss: 0.1390 - val_accuracy: 0.9520\n",
      "Epoch 13/20\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1380 - accuracy: 0.9517 - val_loss: 0.1337 - val_accuracy: 0.9525\n",
      "Epoch 14/20\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1329 - accuracy: 0.9519 - val_loss: 0.1292 - val_accuracy: 0.9528\n",
      "Epoch 15/20\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1291 - accuracy: 0.9524 - val_loss: 0.1253 - val_accuracy: 0.9519\n",
      "Epoch 16/20\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1261 - accuracy: 0.9522 - val_loss: 0.1249 - val_accuracy: 0.9530\n",
      "Epoch 17/20\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1240 - accuracy: 0.9527 - val_loss: 0.1210 - val_accuracy: 0.9525\n",
      "Epoch 18/20\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1225 - accuracy: 0.9524 - val_loss: 0.1195 - val_accuracy: 0.9522\n",
      "Epoch 19/20\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1211 - accuracy: 0.9527 - val_loss: 0.1191 - val_accuracy: 0.9533\n",
      "Epoch 20/20\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1201 - accuracy: 0.9529 - val_loss: 0.1183 - val_accuracy: 0.9538\n"
     ]
    }
   ],
   "source": [
    "# Create a simple neural net which trains on the training data and\n",
    "# confirms the model performance on the validation set\n",
    "w2v_classification_model = train_text_classification_model(\n",
    "    document_vectors_train,\n",
    "    final_labels_train,\n",
    "    document_vectors_val,\n",
    "    final_labels_val,\n",
    "    100,\n",
    "    20,\n",
    "    32\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 0s 576us/step\n",
      "col_0   0.0   1.0\n",
      "row_0            \n",
      "0      3021   183\n",
      "1       113  3083\n"
     ]
    }
   ],
   "source": [
    "# Generate a column in the validation data with the predictions\n",
    "clickbait_val[\"w2v_baseline_pred\"] = generate_predictions(w2v_classification_model,\n",
    "                                                          document_vectors_val,\n",
    "                                                          final_labels_val)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                text\n0  Phoebe Buffay Is Supposed To Die On October 15...\n1  This Body Cam Footage Shows A Vehicle Plow Int...\n2  Ariana Grande Flawlessly Shut Down Sexist Comm...\n3  Photographer Gregory Crewdson Releases Hauntin...\n4  Watch Footage Of Two Sikh Men Unraveling Their...\n5  Joe Biden And Stephen Colbert Have A Remarkabl...\n6  Watch 100 Years Of Brazilian Beauty In A Littl...\n7                7 Struggles Of Taking One More Shot\n8    Stephanie Mills Destroyed Us In NBC's \"The Wiz\"\n9  We Had Pro Gamers Compete Against Vets At A Sh...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Phoebe Buffay Is Supposed To Die On October 15...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>This Body Cam Footage Shows A Vehicle Plow Int...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Ariana Grande Flawlessly Shut Down Sexist Comm...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Photographer Gregory Crewdson Releases Hauntin...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Watch Footage Of Two Sikh Men Unraveling Their...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Joe Biden And Stephen Colbert Have A Remarkabl...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Watch 100 Years Of Brazilian Beauty In A Littl...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>7 Struggles Of Taking One More Shot</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Stephanie Mills Destroyed Us In NBC's \"The Wiz\"</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>We Had Pro Gamers Compete Against Vets At A Sh...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Headlines the model thought were not clickbait, but which are\n",
    "pd.read_csv(f\"{cwd}/data/word2vec_incorrect_prediction_not_clickbait.csv\",\n",
    "            sep = \"\\t\",\n",
    "            header = 0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-07-26T11:37:44.208541Z",
     "start_time": "2023-07-26T11:37:44.179306Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                text\n0                           Where Is Oil Going Next?\n1  With High-Speed Camera, Glimpsing Worlds Too F...\n2          A World of Lingo (Out of This World, Too)\n3       Advertisers Change Game Plans for Super Bowl\n4            Posted deadlines for Christmas delivery\n5  For Refugees, Recession Makes Hard Times Even ...\n6      Samsung + T-Mobile = Phone With a Real Camera\n7                         Sears Tower Is Going Green\n8   Panasonic GH1 Merges S.L.R. Photos With HD Video\n9      TomTom Go 740 Live Has Cellphone Connectivity",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Where Is Oil Going Next?</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>With High-Speed Camera, Glimpsing Worlds Too F...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>A World of Lingo (Out of This World, Too)</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Advertisers Change Game Plans for Super Bowl</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Posted deadlines for Christmas delivery</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>For Refugees, Recession Makes Hard Times Even ...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Samsung + T-Mobile = Phone With a Real Camera</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Sears Tower Is Going Green</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Panasonic GH1 Merges S.L.R. Photos With HD Video</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>TomTom Go 740 Live Has Cellphone Connectivity</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Headlines the model thought were clickbait, but which are not\n",
    "pd.read_csv(f\"{cwd}/data/word2vec_incorrect_prediction_clickbait.csv\",\n",
    "            sep = \"\\t\",\n",
    "            header = 0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2023-07-26T11:37:54.748074Z",
     "start_time": "2023-07-26T11:37:54.740333Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
